<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          hadoop第四课 - 梅老板 | Blog
        
    </title>

    <link rel="canonical" href="http://www.ruozedata.com/article/hadoop第四课/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">

    <link rel="stylesheet" href="/css/donate.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('/img/article_header/article_header.png')
            /*post*/
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#ruozedata" title="ruozedata">ruozedata</a>
                            
                        </div>
                        <h1>hadoop第四课</h1>
                        <h2 class="subheading">This is hexo theme Demo.</h2>
                        <span class="meta">
                            Posted by John on
                            2019-12-10
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">亚历山大-梅西</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <hr>
<p><strong><a href="#id_1" target="_self">一、上次课回顾</a></strong></p>
<p><strong><a href="#id_2" target="_self">二、副本放置策略</a></strong></p>
<ul>
<li><a href="#id_2.1" target="_self">2.1、HDFS写流程</a></li>
<li><a href="#id_2.2" target="_self">2.2、HDFS读流程</a></li>
<li><a href="#id_2.3" target="_self">2.3、hdfs的pid文件</a></li>
<li><a href="#id_2.4" target="_self">2.4、hdfs上的常用命令（回收站机制、dfsadmin）</a></li>
</ul>
<p><strong><a href="#id_3" target="_self">三、本次课程作业</a></strong></p>
<h1 id="一-上次课回顾">一、上次课回顾</h1>
https://alexander-messi.github.io/article/hadoop%E7%AC%AC%E4%B8%89%E8%AF%BE/
<p>回顾：HDFS不适合小文件存储<br>
HDFS的架构，NN、SNN、DN各自的作用，数据块分布在哪些DN节点上<br>
HDFS对于生产上可用的其实大部分都是小文件的解决，常规命令操作，高级班API的开发；对于hdfs的读写流程，架构仅限面试用</p>
<h1 id="二-副本放置策略">二、副本放置策略</h1>
J总公司100台左右的节点，做的也是3副本放置策略；
<p>图解这个流程：有两个机架，Rack1上有4台机器，Rack2上有5台机器，在Rack1机架上的一台机器用来提交文件发现自己这台机器上有DataNode节点，优先会在自己的节点放置一份；第二个副本放在Rack2上的第三台机器上，第三个副本放置在Rack2机架上的第五台机器上。</p>
<p>小结：第一个副本：假如上传节点为DN节点，初始文件优先放置在本节点；上传节点没有DataNode节点，会随机挑选一台磁盘不太慢，CPU不太繁忙的节点；所以第一个副本我们上传的时候尽量挑选有DataNode的节点。</p>
<p>第二个副本：放置在与第一个副本不同的机架的节点上；</p>
<p>第三个副本：放置于第二个副本相同机架的不同节点。</p>
<p>但是正常很多公司如何操作：</p>
<ul>
<li>会单独选择一个节点client node（无namenode角色，无DataNode角色），而是只有xml文件（xml能够在机器间通信）；</li>
<li>正常在公司中使用的也都是CDH，CDH中有一个默认机架（这是一个虚拟概念），我们不调整CDH的默认机架</li>
</ul>
<h1 id="21-hdsf文件写流程">2.1、HDSF文件写流程</h1>
Hdfs dfs -put XXX.log /input，我们在控制台上执行了这个命令之后，它就会去把这个文件上传到hdfs上，这是无感知的。
<p>hdfs client 就相当于是hdfs dfs -put这个命令，在client node上还有Distributed Filesystem和FSData OutputStream；</p>
<p>文件写流程：FSDataOutputStream</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1、Client调用filesystem.create(filepath)方法，与NameNode进行rpc通信，来判定要创建的文件是否存在及是否有权限创建；假如不okay，就返回错误信息；</span><br><span class="line">假如okay，就创建一个新文件，此时不关联任何的block块，返回一个FSDataOutputStream对象（此时数据还没有写）；</span><br><span class="line"></span><br><span class="line">2、Client调用FSDataOutputStream对象的write()方法，先将第一个块的第一个副本写到第一个DN，第一个副本写完；就传输给第二个副本，等待第二个副本写完；就传输给第三个DN，第三个副本写完；</span><br><span class="line">DN3发送ack packet确认包返回给DN2（表示DN3副本已经写完），DN2接收到DN3的确认包+自己本身没问题，返回一个确认包给DN1。DN1再返回确认包给FSDataOutputStream，至此三副本都已经写完。</span><br><span class="line"></span><br><span class="line">3、当向文件写入数据完成后，Client调用FSDataOutputStream对象的close方法，关闭输出流。</span><br><span class="line"></span><br><span class="line">4、再调用分布式的文件系统FileSystem.complete()方法，告诉NameNode写入成功。</span><br></pre></td></tr></table></figure>
<p><img src="http://hadoop001/GithubBlogPicture/hadoop004/HDFS-wirte.png" alt="images"></p>
<h1 id="22-hdsf文件读流程">2.2、HDSF文件读流程</h1>
<p>文件读流程：FSDataInputStream：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1、CLient调用Filesystem.open(filePath)方法，与NameNode进行rpc通信，返回该文件的部分或者全部block列表，也就是返回FSDataInputStream对象。</span><br><span class="line"></span><br><span class="line">2、Client调用FSDataInputStream对象的read方法；</span><br><span class="line">a.与第一个块最近的DN进行通信并且read，读取完成后，会进行check；假如okay，就关闭与当前DataNode进行通信，假如读取失败，会记录失败块的DN信息，下次不会再读取；name会去该块的第二个DN地址上去读取。</span><br><span class="line">理解：block1的三个副本分别在DN1、DN15、DN25这三台机器上，它会读取最近的块。</span><br><span class="line"></span><br><span class="line">b.然后去第二个块的最近的DN上通信读取，check后，关闭通信</span><br><span class="line"></span><br><span class="line">c.假如block列表读取完成后，文件还未结束，就再次FIleSystem会从NN获取该文件的下一批次block列表。（感觉就是连续的数据流，对于客户端操作是透明无感知的）</span><br><span class="line"></span><br><span class="line">3、Client调用FSDataInputStream.close()方法，关闭输入流。</span><br></pre></td></tr></table></figure>
<p><img src="http://hadoop001/GithubBlogPicture/hadoop004/HDFS-read.png" alt="images"></p>
<h1 id="23-hdfs的pid文件">2.3、hdfs的pid文件</h1>
<p>1、vi <a href="http://hadoop-env.sh" target="_blank" rel="noopener">hadoop-env.sh</a>，修改如下这句话：<br>
export HADOOP_PID_DIR=/home/hadoop/tmp</p>
<p>2、vi <a href="http://yarn-env.sh" target="_blank" rel="noopener">yarn-env.sh</a>，添加如下这句话：<br>
export YARN_PID_DIR=/home/hadoop/tmp</p>
<p>重启hdfs发现，pid文件都配置在了我们自己创建的/home/hadoop/tmp目录下：<br>
[hadoop@hadoop001 tmp]$ ll<br>
total 28<br>
drwxrwxr-x 5 hadoop hadoop 4096 Dec  9 15:12 dfs<br>
-rw-rw-r-- 1 hadoop hadoop    6 Dec 11 14:22 hadoop-hadoop-datanode.pid<br>
-rw-rw-r-- 1 hadoop hadoop    6 Dec 11 14:22 hadoop-hadoop-namenode.pid<br>
-rw-rw-r-- 1 hadoop hadoop    6 Dec 11 14:22 hadoop-hadoop-secondarynamenode.pid<br>
drwxr-xr-x 5 hadoop hadoop 4096 Dec 11 14:22 nm-local-dir<br>
-rw-rw-r-- 1 hadoop hadoop    6 Dec 11 14:22 yarn-hadoop-nodemanager.pid<br>
-rw-rw-r-- 1 hadoop hadoop    6 Dec 11 14:22 yarn-hadoop-resourcemanager.pid<br>
[hadoop@hadoop001 tmp]$ pwd<br>
/home/hadoop/tmp</p>
<h1 id="24-hdfs上的常用命令回收站机制-dfsadmin安全模式">2.4、HDFS上的常用命令（回收站机制、dfsadmin安全模式）</h1>
<p>1、hdfs上级联创建文件夹<br>
hdfs dfs -mkdir -p /a/b/c</p>
<p>2、删除1中的级联创建的文件夹：<br>
hdfs dfs -rm -r /a</p>
<h4 id="回收站机制">回收站机制</h4>
<p>3、hdfs上的回收站机制，命令帮助中有如下这一句话：<br>
[-rm [-f] [-r|-R] [-skipTrash] <src> …]</src></p>
<p>所有配置文件只可能在core-site.xml或者hdfs-site.xml中：</p>
<p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml</a></p>
<table>
<thead>
<tr>
<th>name</th>
<th>value</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>fs.trash.interval</td>
<td>0</td>
<td>Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled. This option may be configured both on the server and the client. If trash is disabled server side then the client side configuration is checked. If trash is enabled on the server side then the value configured on the server is used and the client configuration value is ignored.</td>
</tr>
<tr>
<td>fs.trash.checkpoint.interval</td>
<td>0</td>
<td>Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval. Every time the checkpointer runs it creates a new checkpoint out of current and removes checkpoints created more than fs.trash.interval minutes ago.</td>
</tr>
</tbody>
</table>
<p>解析：上图所示：fs.trash.interval的值对应为0的话，回收站机制就是disabled的，我们修改value的值不等于0就意味着回收站机制是开启的，我们修改为100min，意味着过100min回收站数据会被删除一次。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1、我们使用vi core-site.xml，添加一行信息：</span><br><span class="line"> 		&lt;property&gt;</span><br><span class="line">                &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">默认启用回收站机制：</span><br><span class="line">此时直接上传本地文件到hdfs的根目录：</span><br><span class="line">hdfs dfs -put /home/hadoop/data/wordcount.txt /</span><br><span class="line"></span><br><span class="line">2、把wordcount.txt上传到根目录：</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -put /home/hadoop/data/wordcount.txt /wordcount.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3、我们测试删除这条数据，系统会把这条数据移动到trash目录：</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -rm /wordcount.txt</span><br><span class="line">19/12/11 15:21:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/12/11 15:21:38 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://hadoop001:9000/wordcount.txt&apos; to trash at: hdfs://hadoop001:9000/user/hadoop/.Trash/Current/wordcount.txt1576048898384</span><br><span class="line"></span><br><span class="line">注意点：切记检查生产环境是否开启回收站机制，CDH默认是开启的。</span><br><span class="line"></span><br><span class="line">使用命令让它不经过回收站：</span><br><span class="line">4、删除的时候彻底删除，不允许这个文件放到回收站中去：</span><br><span class="line">4.1、还是先上传文件到hdfs的根目录上去，</span><br><span class="line">hdfs dfs -put /home/hadoop/data/wordcount.txt</span><br><span class="line"></span><br><span class="line">4.2、然后使用参数让它跳过回收站直接被删除：</span><br><span class="line">hdfs dfs -rm -skipTrash /wordcount.txt</span><br></pre></td></tr></table></figure>
<p>正常这个参数：fs.trash.interval = 10080(min) = 7天</p>
<p>HDFS上的上传和下载文件（除了常用的put和get以外还有一组命令）：<br>
1、从本地文件系统拷贝到hdfs里：</p>
<ul>
<li>hdfs dfs -copyFromLocal /input/hello.txt /input/<br>
2、从hdfs拷贝到本地：<br>
hdfs dfs -copyToLocal /input/hello.txt /input</li>
</ul>
<h4 id="dfsadmin命令">dfsadmin命令：</h4>
<p>如果NN log显示进入safe mode，正常手动让其离开安全模式：</p>
<p>1、开启安全模式：<br>
hdfs dfsadmin -safemode enter</p>
<p>2、在安全模式下只能读取数据，无法上传数据：</p>
<h4 id="hdfs-fsck查看节点安全情况">hdfs fsck（查看节点安全情况）：</h4>
<p>hdfs fsck /				从根目录开始查看健康状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">注意：检查健康状态的命令</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://hadoop001:50070/fsck?ugi=hadoop&amp;path=%2F</span><br><span class="line">FSCK started by hadoop (auth:SIMPLE) from /172.16.56.88 for path / at Wed Dec 11 16:13:28 CST 2019</span><br><span class="line">.............Status: HEALTHY</span><br><span class="line"> Total size:    434705294 B</span><br><span class="line"> Total dirs:    17</span><br><span class="line"> Total files:   13</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      15 (avg. block size 28980352 B)</span><br><span class="line"> Minimally replicated blocks:   15 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    1</span><br><span class="line"> Average block replication:     1.0</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              0 (0.0 %)</span><br><span class="line"> Number of data-nodes:          1</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Wed Dec 11 16:13:28 CST 2019 in 21 milliseconds</span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br></pre></td></tr></table></figure>
<p>拓展：<br>
log日志文件 --&gt;使用flume抽取到hdfs上，DB使用SQOOP、DATAX、CANAL、MAXWELL抽取数据到hdfs上；我们不应该在这些插件写入的时候在hdfs上开启安全模式，使得这些数据无法导入；正确的做法应该是在上游的时候就做一个开关。</p>
<p>原因：HDFS集群故障的时候启动NameNode的时候会抛错，它会进入安全模式，故障排查完后，如何手动离开安全模式，hdfs dfsadmin -safemode leave.</p>
<p>hadoop常用命令如下：<br>
<a href="https://blog.csdn.net/zhikanjiani/article/details/100039072" target="_blank" rel="noopener">https://blog.csdn.net/zhikanjiani/article/details/100039072</a></p>
<h4 id="df-h">df -h</h4>
<ul>
<li>三台机器，DN1节点磁盘已使用空间90%，DN2节点磁盘使用空间60%，DN3节点的磁盘使用空间70%，思考问题：各DN节点的数据均衡问题该如何处理？</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1、路径位置：</span><br><span class="line">[hadoop@hadoop001 sbin]$ pwd</span><br><span class="line">/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/sbin</span><br><span class="line"></span><br><span class="line">2、执行这个脚本：</span><br><span class="line">[hadoop@hadoop001 sbin]$ ./start-balancer.sh</span><br><span class="line">starting balancer, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-balancer-hadoop001.out</span><br><span class="line"></span><br><span class="line">3、查看这个脚本：</span><br><span class="line">[hadoop@hadoop001 sbin]$ cat start-balancer.sh</span><br><span class="line"># Start balancer daemon.</span><br><span class="line">&quot;$HADOOP_PREFIX&quot;/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script &quot;$bin&quot;/hdfs start balancer $@</span><br><span class="line"></span><br><span class="line">4、我们去查看这个shell脚本产生的日志信息：</span><br><span class="line">[hadoop@hadoop001 logs]$ cat hadoop-hadoop-balancer-hadoop001.log</span><br><span class="line">2019-12-11 16:20:54,041 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: namenodes  = [hdfs://hadoop001:9000]</span><br><span class="line">2019-12-11 16:20:54,077 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: parameters = Balancer.Parameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, run during upgrade = false]</span><br><span class="line">2019-12-11 16:20:54,077 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: included nodes = []</span><br><span class="line">2019-12-11 16:20:54,077 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: excluded nodes = []</span><br><span class="line">2019-12-11 16:20:54,077 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: source nodes = []</span><br><span class="line">2019-12-11 16:20:54,302 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-12-11 17:06:29,693 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: namenodes  = [hdfs://localhost:9000]</span><br><span class="line">2019-12-11 17:06:29,696 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: parameters = Balancer.Parameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, run during upgrade = false]</span><br><span class="line">2019-12-11 17:06:29,696 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: included nodes = []</span><br><span class="line">2019-12-11 17:06:29,696 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: excluded nodes = []</span><br><span class="line">2019-12-11 17:06:29,696 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: source nodes = []</span><br></pre></td></tr></table></figure>
<p>我们在这个文件中找到两个与之相关的参数：<br>
1、threshold  = 10.0（阈值）</p>
<p>2、90+60+80=230/3=76%</p>
<ul>
<li>所有节点的磁盘used与集群的平均used之差要小于这个阈值。</li>
</ul>
<p>90-76=14<br>
60-76=-16<br>
80-76=4，需要做数据迁移，意思就是每个节点的使用百分比要小于76%；</p>
<ul>
<li>dfs.datanode.balance.bandwidthPersec 的value值J总设置的是30m，执行时候的shell脚本：…start-balancer.sh，每天晚上都写个shell脚本进行调度 --&gt; crontab job每天凌晨20分进行调度。</li>
</ul>
<p>一个DN节点的多个磁盘的数据均衡，每天调度shell脚本，做了数据平衡，进行毛刺修正；确保磁盘的数据维护在一个区间中。</p>
<ul>
<li>一个DN节点的多个磁盘的数据均衡，df -h<br>
/data01	90%<br>
/data02 60%<br>
/data03 80%<br>
此时又加了一块新磁盘：/data04 0%，如何去做，CDH版本中就已经支持了，在Apache版本中是不支持这种做法的。</li>
</ul>
<p>如何去做：在hdfs-site.xml中通过参数设置：set dfs.disk.balancer.enabled<br>
三步走：<br>
第一步：生成执行计划：hdfs diskbalancer -plan ruozedata001		生成ruozedata001.plan.json<br>
第二步：hdfs diskbalancer -execute /system/diskbalancer/namenode.plan.json		执行<br>
第三步：hdfs diskbalancer -query ruozedata001		查询状态</p>
<p>什么时候去手动或调度执行？<br>
a.新磁盘加入的情况下<br>
b.监控服务器的磁盘剩余空间，小于阈值 10%，发预警邮件，我们手动执行。</p>
<p>CDH中关于新加磁盘应该如何进行配置：</p>
<ul>
<li>dfs.datanode.data.dir  /data,/data,/data03,/data04        就通过这个参数进行配置，新加盘就这样,/data04这样的形式添加。</li>
<li>为什么DN的生产上挂载多个物理的磁盘目录：<br>
/data01 disk01<br>
/data02 disk02<br>
/data03 disk03<br>
此时来了一份数据，机器上有三块磁盘，这三块磁盘都作为DataNode的目录，三块磁盘的IO；第一块磁盘100m每秒，第二块磁盘100m每秒，第三块磁盘100m每秒。<br>
主要原因是为了高效率的读和写。提前规划好2~3年的数据存储量。</li>
</ul>
<p>J总公司148个生产节点，每个节点10块1W转的硬盘，一台服务器就是10T，没有做raid；避免后期手工加磁盘维护的工作量。</p>
<h1 id="三-作业">三、作业</h1>
1、HDFS的读写流程、副本放置策略
<p>2、pid文件</p>
<p>3、hdfs dfs常用命令</p>
<p>4、多节点、单节点的磁盘均衡</p>
<p>5、整理安全模式，一定要在上游做一个开关，保障数据零丢失；否则我们都不知道数据流出了多少？</p>
<p>面试题1：FSDataINputStream和FSDataOutputStream分别对应的是什么？</p>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/article/hadoop第五课/" data-toggle="tooltip" data-placement="top" title="hadoop第五课">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/article/大数据调度平台rundeck/" data-toggle="tooltip" data-placement="top" title="大数据调度平台rundeck">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <br>

                <!--打赏-->
                
                <!--打赏-->

                <br>
                <!--分享-->
                
                    <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                    <!--  css & js -->
                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!--分享-->
                <br>                       
                
                <!-- require APlayer -->
                

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->

                

            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

  
    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#一-上次课回顾"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">&#x4E00;&#x3001;&#x4E0A;&#x6B21;&#x8BFE;&#x56DE;&#x987E;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#二-副本放置策略"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">&#x4E8C;&#x3001;&#x526F;&#x672C;&#x653E;&#x7F6E;&#x7B56;&#x7565;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#21-hdsf文件写流程"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">2.1&#x3001;HDSF&#x6587;&#x4EF6;&#x5199;&#x6D41;&#x7A0B;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#22-hdsf文件读流程"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">2.2&#x3001;HDSF&#x6587;&#x4EF6;&#x8BFB;&#x6D41;&#x7A0B;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#23-hdfs的pid文件"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">2.3&#x3001;hdfs&#x7684;pid&#x6587;&#x4EF6;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#24-hdfs上的常用命令回收站机制-dfsadmin安全模式"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">2.4&#x3001;HDFS&#x4E0A;&#x7684;&#x5E38;&#x7528;&#x547D;&#x4EE4;&#xFF08;&#x56DE;&#x6536;&#x7AD9;&#x673A;&#x5236;&#x3001;dfsadmin&#x5B89;&#x5168;&#x6A21;&#x5F0F;&#xFF09;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#回收站机制"><span class="toc-nav-number">6.0.0.1.</span> <span class="toc-nav-text">&#x56DE;&#x6536;&#x7AD9;&#x673A;&#x5236;</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#dfsadmin命令"><span class="toc-nav-number">6.0.0.2.</span> <span class="toc-nav-text">dfsadmin&#x547D;&#x4EE4;&#xFF1A;</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#hdfs-fsck查看节点安全情况"><span class="toc-nav-number">6.0.0.3.</span> <span class="toc-nav-text">hdfs fsck&#xFF08;&#x67E5;&#x770B;&#x8282;&#x70B9;&#x5B89;&#x5168;&#x60C5;&#x51B5;&#xFF09;&#xFF1A;</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#df-h"><span class="toc-nav-number">6.0.0.4.</span> <span class="toc-nav-text">df -h</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#三-作业"><span class="toc-nav-number">7.</span> <span class="toc-nav-text">&#x4E09;&#x3001;&#x4F5C;&#x4E1A;</span></a></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#ruozedata" title="ruozedata">ruozedata</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="http://www.ruozedata.com" target="_blank">若泽数据官网</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>








<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'rz'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


<!-- chrome Firefox 中文锚点定位失效-->
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- smooth scroll behavior polyfill  -->
<script type="text/javascript" src="/js/smoothscroll.js"></script>
<script>
        $('#toc').on('click','a',function(a){
            // var isChrome = window.navigator.userAgent.indexOf("Chrome") !== -1;
            // console.log(window.navigator.userAgent,isChrome)
                // if(isChrome) {
                    // console.log(a.currentTarget.outerHTML);
                    // console.log($(a.currentTarget).attr("href"));
                    //跳转到指定锚点
                    // document.getElementById(a.target.innerText.toLowerCase()).scrollIntoView(true);
                    document.getElementById($(a.currentTarget).attr("href").replace("#","")).scrollIntoView({behavior: 'smooth' });
                // }
        })  
</script>


    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/hackeruncle">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; John 2020 
                    By <a href="http://www.ruozedata.com">若泽数据，企业在职</a> | BigData
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=ruozedata&repo=Bigdata&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://www.ruozedata.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'xxx';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="http://www.ruozedata.com/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
